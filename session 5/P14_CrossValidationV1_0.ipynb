{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "desperate-printer",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "## A first way to train a supervised learning model\n",
    "\n",
    "The data used in supervised learning tasks usually contains several features (or independent variables) and a single label (or dependent variable) for a set of observations. <br>\n",
    "\n",
    "A supervised ML algorithm try to model the relationship between features and label. <br>\n",
    "We first train the model by providing both features and label for some observations. Then test the model by only providing features and expecting it to predict the labels. Thus, we need to split the data into training and test subsets. <br>\n",
    "We let the model to learn on training set and then measure its performance on test set.<br>\n",
    "\n",
    "Scikit-learn library provides several tools to split data into training and test sets.<br>\n",
    "The most basic one is **`train_test_split()`** which just divides the data into two parts according to the specified partitioning ratio: <br><br>\n",
    "`train_test_split(test_size=0.2)` will set aside 20% of the data for testing and 80% for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controversial-flour",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "            19],\n",
      "           dtype='int64') TEST: Int64Index([20, 21, 22, 23, 24], dtype='int64')\n",
      "TRAIN: 0     0.066609\n",
      "1     0.168764\n",
      "2     0.245469\n",
      "3     0.912774\n",
      "4     0.806911\n",
      "5     0.338475\n",
      "6     0.771831\n",
      "7     0.631860\n",
      "8     0.496587\n",
      "9     0.150872\n",
      "10    0.222878\n",
      "11    0.181049\n",
      "12    0.040606\n",
      "13    0.757292\n",
      "14    0.465324\n",
      "15    0.880543\n",
      "16    0.491598\n",
      "17    0.472164\n",
      "18    0.048185\n",
      "19    0.689647\n",
      "Name: feature, dtype: float64 TEST: 20    0.327801\n",
      "21    0.692204\n",
      "22    0.651774\n",
      "23    0.365881\n",
      "24    0.828829\n",
      "Name: feature, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = np.ones(25)\n",
    "target[-5:] = 0\n",
    "df = pd.DataFrame({'feature':np.random.random(25),\n",
    "                  'target':target})\n",
    "X = df.feature\n",
    "y = df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "print(\"TRAIN:\", X_train.index, \"TEST:\", X_test.index)\n",
    "print(\"TRAIN:\", X_train, \"TEST:\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-baker",
   "metadata": {},
   "source": [
    "If we set the **`shuffle`** parameter to `True` (the default value), the data will be randomly splitted. <br>\n",
    "\n",
    "If we want the splits to be reproducible, we also need to pass in an integer to **`random_state`** parameter. Otherwise, each time we run `train_test_split()`, different indices will be splitted into training and test set.<br>\n",
    "\n",
    "## How to improve the way the model is trained\n",
    "\n",
    "Data is a valuable asset and we want to make use of every bit of it. If we split data using `train_test_split()`, we can only train a model with the portion set aside for training. <br>\n",
    "The models get better as the amount of training data increases. <br>\n",
    "\n",
    "One solution to overcome this issue is **cross validation**. With cross validation, the dataset is divided into N splits. <br>\n",
    "N-1 splits are used for training and the remaining split is used for testing. The model runs through the entire dataset N times and at each time, a different split is used for testing. Thus, we use all of data points for both training and testing. <br>\n",
    "\n",
    "Cross validation is also useful to measure the performance of a model more accurately, especially on new, previously unseen data points.<br>\n",
    "\n",
    "There are different methods to split data in cross validation: **KFold** and **StratifiedKFold** are commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-pitch",
   "metadata": {},
   "source": [
    "## KFold\n",
    "\n",
    "As the name suggests, **KFold** divides the dataset into `k` folds (`k` refers to the number of groups that a given data sample is to be split into). <br>\n",
    "\n",
    "If **`shuffle`** is set to `False`, consecutive folds will be the shifted version of previous fold. <br>\n",
    "\n",
    "At first iteration, test set is the first four indices. Then `KFold` keeps shifting the test set `k` times. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informal-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] TEST: [0 1 2 3 4 5 6]\n",
      "TRAIN: [ 0  1  2  3  4  5  6 13 14 15 16 17 18 19 20 21 22 23 24] TEST: [ 7  8  9 10 11 12]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 19 20 21 22 23 24] TEST: [13 14 15 16 17 18]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18] TEST: [19 20 21 22 23 24]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = df.feature\n",
    "y = df.target\n",
    "\n",
    "kf = KFold(n_splits=4)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-trinidad",
   "metadata": {},
   "source": [
    "If shuffle is set to True, then the splitting will be random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-future",
   "metadata": {},
   "source": [
    "We can then make use of the sample (TRAIN), such as to evaluate the skill of a machine learning algorithm.<br>\n",
    "\n",
    "Here, four models are trained and evaluated with each fold given a chance to be the held out test set.<br>\n",
    "\n",
    "For example:\n",
    "\n",
    "Model1: Trained on [ 0  1  2  4  5  6  7  8  9 11 12 15 16 19 20 22 23 24], Tested on [ 3 10 13 14 17 18 21]\n",
    "<br>\n",
    "Model2: Trained on [ 0  1  3  5  8  9 10 11 12 13 14 15 16 17 18 21 22 23 24], Tested on [ 2  4  6  7 19 20]<br>\n",
    "etc ...\n",
    "\n",
    "The models are then discarded after they are evaluated as they have served their purpose.<br>\n",
    "\n",
    "The skill scores are collected for each model and summarized for use.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expired-tennis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********************************************************************************\n",
      "The cross-validation scores using custom method are \n",
      "[0.9666666666666667, 0.9666666666666667, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333]\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "Mean of k-fold scores using custom method is 0.9466666666666667\n",
      "********************************************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import datasets from sklearn library\n",
    "from sklearn import datasets\n",
    "data = datasets.load_iris()\n",
    " \n",
    "#Import decision tree classification model and cross validation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    " \n",
    "#Get input and output datasets values in X and Y variables\n",
    "X = data.data\n",
    "y = data.target\n",
    " \n",
    "#Initialize k-fold cross validation configurations\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    " \n",
    "scores = []\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth = 2, \n",
    "                            min_samples_leaf = 0.10, random_state = 42)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    " \n",
    "    dt.fit(X_train, y_train)\n",
    "    scores.append(dt.score(X_test, y_test))\n",
    "    \n",
    "print(\"\\n\") \n",
    "print(\"*\" * 80)\n",
    "print(f\"The cross-validation scores using custom method are \\n{scores}\")\n",
    "print(\"*\" * 80)\n",
    " \n",
    "import numpy as np\n",
    "print(\"\\n\") \n",
    "print(\"*\" * 80)\n",
    "print(f\"Mean of k-fold scores using custom method is {np.mean(scores)}\")\n",
    "print(\"*\" * 80)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-quilt",
   "metadata": {},
   "source": [
    "## StratifiedKFold\n",
    "\n",
    "**StratifiedKFold** takes the cross validation one step further. The class distribution in the dataset is preserved in the training and test splits.<br>\n",
    "\n",
    "In our very simple DataFrame, there are 16 data points. <br>\n",
    "12 of them belong to class 1 and the remaining 4 belong to class 0 so this is an \"imbalanced\" class distribution. <br>\n",
    "\n",
    "**KFold** does not take this into consideration.<br> \n",
    "Therefore, in classifications tasks with imbalanced class distributions, we should prefer **StratifiedKFold** over **KFold**.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-collectible",
   "metadata": {},
   "source": [
    "## k-fold cross-validation using built-in cross_val_score method\n",
    "\n",
    "Instead of using a custom code, we can also use the built-in **`cross_val_score()`** method of the sklearn library. <br>\n",
    "\n",
    "`cross_val_score()` does the same thing as our previous example: it takes an estimator, the features and target, splits into k-folds (`k` which is the **cv** parameter, 5 by default), fits on the (k-1) folds and evaluates on the last fold.<br> \n",
    "It does this `k` times and performance measure such as \"accuracy\" is computed at each step.<br>\n",
    "\n",
    "Below is a sample code which shows how we can use this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "express-spirituality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "The cross-validation scores using custom method are \n",
      "[0.93333333 0.96666667 0.9        0.86666667 1.        ]\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Mean of k-fold scores using custom method is 0.9333333333333332\n",
      "****************************************************************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import datasets from sklearn library\n",
    "from sklearn import datasets\n",
    "data = datasets.load_iris()\n",
    " \n",
    "#Import decision tree classification model and cross validation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Get input and output datasets values in X and Y variables\n",
    "X = data.data\n",
    "y = data.target\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth = 2, \\\n",
    "                        min_samples_leaf = 0.10, random_state = 42)\n",
    " \n",
    "scores = cross_val_score(dt, X, y, cv = 5)\n",
    "\n",
    "print(\"\\n\") \n",
    "print(\"*\" * 100)\n",
    "print(f\"The cross-validation scores using custom method are \\n{scores}\")\n",
    "print(\"*\" * 100)\n",
    " \n",
    "import numpy as np\n",
    "print(\"\\n\") \n",
    "print(\"*\" * 100)\n",
    "print(f\"Mean of k-fold scores using custom method is {np.mean(scores)}\")\n",
    "print(\"*\" * 100)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-concrete",
   "metadata": {},
   "source": [
    "## How to choose the K value\n",
    "\n",
    "The **k** value must be chosen carefully for your data sample (a poorly chosen value for k may result in a mis-representative idea of the skill of the model).\n",
    "\n",
    "Three common tactics for choosing a value for `k` are as follows:\n",
    "\n",
    "1. The value for `k` is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.\n",
    "2. The value for `k` is fixed to **10**: this is very common in the field of applied machine learning, and is recommend if you are struggling to choose a value for your dataset.\n",
    "3. The value for `k` is fixed to **n**, where **n** is the size of the dataset to give each test sample an opportunity to be used in the hold out dataset. \n",
    "\n",
    "\n",
    "**Note**: most of the time, one performs k-fold cross-validation using `k = 5` or `k = 10`, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\n",
    "\n",
    "**Note**: If a value for `k` is chosen that does not evenly split the data sample, then one group will contain a remainder of the examples: it is preferable to split the data sample into `k` groups with the same number of samples.\n",
    "\n",
    "**Note**: Be cautious in choosing the value of hyperparameter `k`: as the number of `k` increases, the model becomes more computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-manual",
   "metadata": {},
   "source": [
    "## To summarize\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data.<br>\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into k groups\n",
    "3. For each unique group:\n",
    "    1. Take the group as a hold out or test data set\n",
    "    2. Take the remaining groups as a training data set\n",
    "    3. Fit a model on the training set and evaluate it on the test set\n",
    "    4. Retain the evaluation score and discard the model\n",
    "4. Summarize the skill of the model using the sample of model evaluation scores<br>\n",
    "\n",
    "Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
